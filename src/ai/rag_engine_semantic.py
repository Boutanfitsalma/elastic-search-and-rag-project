"""
RAG Engine S√âMANTIQUE - Version AM√âLIOR√âE
Corrections : anti-hallucination, filtres, agr√©gations
"""
import json
import requests
from elasticsearch import Elasticsearch
from sentence_transformers import SentenceTransformer
from functools import lru_cache
from datetime import datetime

# Config
ES_HOST = 'localhost:9200'
ES_INDEX = 'mozilla-ci-logs-semantic-v2'
OLLAMA_API = 'http://localhost:11434/api/generate'
OLLAMA_MODEL = 'mistral'

print("üß† Chargement du mod√®le d'embeddings...")
model = SentenceTransformer('all-MiniLM-L6-v2')
print("‚úÖ Mod√®le charg√©")

es = Elasticsearch([f'http://{ES_HOST}'])

# Historique
conversation_history = []

# ============================================================================
# KEYWORDS AM√âLIOR√âS (avec conjugaisons)
# ============================================================================

FILTER_KEYWORDS = {
    "status": {
        "failure": ["√©chou√©", "√©chouent", "√©chec", "√©choue", "failed", "failure", "erreur", "erreurs"],
        "success": ["r√©ussi", "r√©ussit", "success", "ok", "succ√®s"]
    },
    "platform": {
        "linux": ["linux", "ubuntu"],
        "windows": ["windows", "win"],
        "mac": ["mac", "osx", "macos"]
    }
}

# Patterns pour d√©tecter les questions d'agr√©gation
AGGREGATION_PATTERNS = {
    "top_builders_errors": ["builders avec le plus d'erreurs", "top builders erreurs", "builders les plus d'erreurs"],
    "average_duration": ["dur√©e moyenne", "temps moyen", "average duration"],
    "count_by_platform": ["nombre par plateforme", "r√©partition plateforme"],
}

# ============================================================================
# FONCTIONS HISTORIQUE
# ============================================================================

def add_to_history(question, answer, results_count):
    """Ajoute √† l'historique"""
    conversation_history.append({
        'timestamp': datetime.now().isoformat(),
        'question': question,
        'answer': answer,
        'results_count': results_count
    })
    if len(conversation_history) > 10:
        conversation_history.pop(0)

def get_conversation_context():
    """Contexte historique"""
    if not conversation_history:
        return ""
    context = "\nHISTORIQUE (3 derni√®res questions):\n"
    for i, item in enumerate(conversation_history[-3:], 1):
        context += f"Q{i}: {item['question']}\n"
        context += f"R{i}: {item['answer'][:100]}...\n"
    return context

def clear_history():
    global conversation_history
    conversation_history = []
    print("üóëÔ∏è  Historique effac√©")

# ============================================================================
# EMBEDDINGS
# ============================================================================

@lru_cache(maxsize=100)
def get_cached_embedding(query):
    """Cache embeddings"""
    return model.encode(query).tolist()

# ============================================================================
# D√âTECTION DE FILTRES (AM√âLIOR√â)
# ============================================================================

def detect_filters(query_lower):
    """
    D√©tecte automatiquement les filtres depuis la question
    Retourne une liste de filtres ES
    """
    filters = []
    
    # Filtre STATUS
    for status, keywords in FILTER_KEYWORDS["status"].items():
        if any(kw in query_lower for kw in keywords):
            filters.append({"term": {"metadata.result_status": status}})
            break  # Un seul statut √† la fois
    
    # Filtre PLATFORM
    for platform, keywords in FILTER_KEYWORDS["platform"].items():
        if any(kw in query_lower for kw in keywords):
            filters.append({"term": {"context.platform": platform}})
            break  # Une seule plateforme √† la fois
    
    return filters

# ============================================================================
# D√âTECTION DE QUESTIONS D'AGR√âGATION
# ============================================================================

def detect_aggregation_query(query_lower):
    """
    D√©tecte si la question n√©cessite une agr√©gation ES plut√¥t que du RAG
    Retourne le type d'agr√©gation ou None
    """
    for agg_type, patterns in AGGREGATION_PATTERNS.items():
        if any(pattern in query_lower for pattern in patterns):
            return agg_type
    return None

# ============================================================================
# AGR√âGATIONS ELASTICSEARCH
# ============================================================================

def get_top_builders_by_errors(limit=5):
    """
    Retourne les top N builders avec le plus d'erreurs (agr√©gation ES)
    """
    agg_query = {
        "size": 0,
        "aggs": {
            "top_builders": {
                "terms": {
                    "field": "metadata.builder.keyword",
                    "order": {"total_errors": "desc"},
                    "size": limit
                },
                "aggs": {
                    "total_errors": {
                        "sum": {"field": "errors.error_count"}
                    }
                }
            }
        }
    }
    
    try:
        results = es.search(index=ES_INDEX, body=agg_query)
        buckets = results['aggregations']['top_builders']['buckets']
        
        response = f"Les {limit} builders avec le plus d'erreurs sont :\n"
        for i, bucket in enumerate(buckets, 1):
            builder = bucket['key']
            total_errors = int(bucket['total_errors']['value'])
            response += f"{i}. {builder} : {total_errors:,} erreurs\n"
        
        return response.strip()
    except Exception as e:
        return f"‚ùå Erreur agr√©gation: {e}"

def get_average_duration():
    """
    Calcule la vraie dur√©e moyenne globale (agr√©gation ES)
    """
    agg_query = {
        "size": 0,
        "aggs": {
            "avg_duration": {
                "avg": {"field": "metrics.duration_seconds"}
            },
            "count": {
                "value_count": {"field": "metrics.duration_seconds"}
            }
        }
    }
    
    try:
        results = es.search(index=ES_INDEX, body=agg_query)
        avg = results['aggregations']['avg_duration']['value']
        count = results['aggregations']['count']['value']
        
        minutes = int(avg // 60)
        seconds = int(avg % 60)
        
        response = f"La dur√©e moyenne globale des builds est de {avg:.0f} secondes "
        response += f"({minutes} minutes et {seconds} secondes), "
        response += f"calcul√©e sur {count:,} builds."
        
        return response
    except Exception as e:
        return f"‚ùå Erreur agr√©gation: {e}"

# ============================================================================
# RECHERCHE S√âMANTIQUE
# ============================================================================

def smart_search(query, limit=5, use_hybrid=True):
    """
    Recherche avec filtres automatiques
    """
    query_lower = query.lower()
    filters = detect_filters(query_lower)
    query_embedding = get_cached_embedding(query)
    
    if use_hybrid:
        # Mode hybride
        search_query = {
            "size": limit,
            "query": {
                "bool": {
                    "should": [
                        {
                            "multi_match": {
                                "query": query,
                                "fields": ["text_content^2", "metadata.builder"],
                                "boost": 1.0
                            }
                        }
                    ]
                }
            },
            "knn": {
                "field": "embedding",
                "query_vector": query_embedding,
                "k": limit,
                "num_candidates": 200,
                "boost": 2.5
            },
            "_source": {
                "excludes": ["embedding"]
            }
        }
        
        # Ajouter filtres SEULEMENT s'ils existent
        if filters:
            search_query["query"]["bool"]["filter"] = filters
            search_query["knn"]["filter"] = filters
    else:
        # S√©mantique pur
        search_query = {
            "knn": {
                "field": "embedding",
                "query_vector": query_embedding,
                "k": limit,
                "num_candidates": 200
            },
            "_source": {
                "excludes": ["embedding"]
            }
        }
        
        if filters:
            search_query["knn"]["filter"] = filters
    
    try:
        results = es.search(index=ES_INDEX, body=search_query)
        return results
    except Exception as e:
        print(f"‚ùå Erreur recherche: {e}")
        # Fallback: recherche simple sans filtres
        try:
            simple_query = {
                "knn": {
                    "field": "embedding",
                    "query_vector": query_embedding,
                    "k": limit,
                    "num_candidates": 200
                },
                "_source": {"excludes": ["embedding"]}
            }
            return es.search(index=ES_INDEX, body=simple_query)
        except:
            return {"hits": {"hits": []}}

# ============================================================================
# FORMATAGE CONTEXTE
# ============================================================================

def format_context_compact(es_results):
    """
    Contexte COMPACT pour √©viter timeout Ollama
    """
    context_parts = []
    
    for i, hit in enumerate(es_results['hits']['hits'], 1):
        src = hit['_source']
        m = src.get('metadata', {})
        met = src.get('metrics', {})
        err = src.get('errors', {})
        
        # Format ultra-compact
        doc = f"{i}. {m.get('builder', 'N/A')[:40]} | "
        doc += f"{m.get('result_status', 'N/A')} | "
        doc += f"{met.get('duration_seconds', 0):.0f}s | "
        doc += f"CPU:{met.get('cpu_user', 0):.0f}% | "
        doc += f"Err:{err.get('error_count', 0)}"
        
        context_parts.append(doc)
    
    return "\n".join(context_parts)

# ============================================================================
# LLM GENERATION (PROMPT AM√âLIOR√â)
# ============================================================================

def ask_ollama_improved(question, context, conversation_context):
    """
    Prompt RENFORC√â contre les hallucinations
    """
    system_prompt = f"""Tu es un expert CI/CD Mozilla. 

R√àGLES STRICTES:
1. Base-toi UNIQUEMENT sur les 5 logs ci-dessous
2. Ne cite QUE des informations pr√©sentes dans ces logs
3. Si tu ne peux pas r√©pondre pr√©cis√©ment, DIS "Information insuffisante dans ces 5 logs"
4. Si la question demande une moyenne/statistique globale, pr√©cise "sur ces 5 logs uniquement"
5. Ne dis JAMAIS "probablement" ou "possiblement" sans preuve dans les logs
6. R√©ponds en 3-5 phrases maximum

LOGS (5 builds analys√©s):
{context}

{conversation_context}

QUESTION: {question}

R√âPONSE (bas√©e UNIQUEMENT sur les 5 logs ci-dessus):"""

    payload = {
        "model": OLLAMA_MODEL,
        "prompt": system_prompt,
        "stream": False,
        "options": {
            "temperature": 0.1,
            "num_predict": 200
        }
    }
    
    try:
        response = requests.post(OLLAMA_API, json=payload, timeout=180)
        return response.json().get('response', "Erreur Ollama")
    except requests.exceptions.Timeout:
        return "‚è±Ô∏è Timeout Ollama (>3min). R√©duisez la complexit√© de la question."
    except Exception as e:
        return f"‚ùå Erreur Ollama: {e}"

# ============================================================================
# FONCTION PRINCIPALE
# ============================================================================

def ask_question_optimized(question, search_mode="hybrid", show_details=True):
    """
    Orchestration intelligente : RAG vs Agr√©gation
    """
    print(f"üîç Analyse de la question: {question}")
    
    if question.lower() in ['clear', 'reset', 'effacer']:
        clear_history()
        return "Historique effac√©"
    
    query_lower = question.lower()
    
    # ========================================================================
    # √âTAPE 1 : D√©tecter si agr√©gation n√©cessaire
    # ========================================================================
    agg_type = detect_aggregation_query(query_lower)
    
    if agg_type == "top_builders_errors":
        print("üìä D√©tection : Agr√©gation (top builders par erreurs)")
        answer = get_top_builders_by_errors(limit=5)
        print(f"\nü§ñ R√âPONSE:\n{answer}\n")
        add_to_history(question, answer, 0)
        return answer
    
    elif agg_type == "average_duration":
        print("üìä D√©tection : Agr√©gation (dur√©e moyenne globale)")
        answer = get_average_duration()
        print(f"\nü§ñ R√âPONSE:\n{answer}\n")
        add_to_history(question, answer, 0)
        return answer
    
    # ========================================================================
    # √âTAPE 2 : RAG classique (recherche s√©mantique)
    # ========================================================================
    print(f"üîç Recherche {search_mode} s√©mantique...")
    use_hybrid = (search_mode == "hybrid")
    results = smart_search(question, limit=5, use_hybrid=use_hybrid)
    
    if not results['hits']['hits']:
        answer = "Aucun log pertinent trouv√©. Essayez de reformuler votre question."
        print(f"‚ùå {answer}")
        return answer
    
    results_count = len(results['hits']['hits'])
    print(f"‚úÖ {results_count} logs pertinents trouv√©s")
    
    if show_details:
        print("\nüìä Top r√©sultats:")
        for i, hit in enumerate(results['hits']['hits'][:5], 1):
            builder = hit['_source'].get('metadata', {}).get('builder', 'N/A')
            score = hit.get('_score', 0)
            status = hit['_source'].get('metadata', {}).get('result_status', 'N/A')
            duration = hit['_source'].get('metrics', {}).get('duration_seconds', 0)
            print(f"  {i}. [{status}] {builder[:45]}... ({duration:.0f}s, score:{score:.2f})")
        print()
    
    # Contexte compact
    context = format_context_compact(results)
    conversation_context = get_conversation_context()
    
    # Ollama avec prompt am√©lior√©
    print("üß† G√©n√©ration de la r√©ponse (Mistral)...")
    answer = ask_ollama_improved(question, context, conversation_context)
    
    print(f"\nü§ñ R√âPONSE:\n{answer}")
    print()
    
    add_to_history(question, answer, results_count)
    return answer

# ============================================================================
# STATISTIQUES
# ============================================================================

def show_stats():
    """Stats session"""
    print("\n" + "="*80)
    print("üìä STATISTIQUES DE SESSION")
    print("="*80)
    print(f"Questions pos√©es: {len(conversation_history)}")
    if conversation_history:
        avg = sum(item['results_count'] for item in conversation_history) / len(conversation_history)
        print(f"Moyenne r√©sultats/question: {avg:.1f} logs")
    print("="*80 + "\n")

# ============================================================================
# MODE INTERACTIF
# ============================================================================

if __name__ == "__main__":
    print("\n" + "="*80)
    print("ü§ñ RAG S√âMANTIQUE - Mozilla CI (VERSION AM√âLIOR√âE)")
    print("="*80)
    print("\nüí° Commandes:")
    print("  - 'clear' : Effacer historique")
    print("  - 'stats' : Statistiques")
    print("  - 'quit' : Quitter")
    print("\nüìù Questions exemples:")
    print("  1. Quels sont les 5 builds les plus lents ?")
    print("  2. Pourquoi les builds √©chouent-ils ?")
    print("  3. Compare Windows vs Linux")
    print("  4. Quels sont les 5 builders avec le plus d'erreurs ?  [AGR√âGATION]")
    print("  5. Quelle est la dur√©e moyenne des builds ?  [AGR√âGATION]")
    print("\n" + "="*80 + "\n")
    
    while True:
        try:
            question = input("‚ùì Question: ").strip()
            
            if question.lower() in ['quit', 'exit', 'q']:
                show_stats()
                print("üëã Au revoir !")
                break
            
            if question.lower() == 'stats':
                show_stats()
                continue
            
            if not question:
                continue
            
            print()
            ask_question_optimized(question, search_mode="hybrid", show_details=True)
            
        except KeyboardInterrupt:
            print("\n\nüëã Arr√™t")
            show_stats()
            break
        except Exception as e:
            print(f"\n‚ùå Erreur: {e}")
            import traceback
            traceback.print_exc()